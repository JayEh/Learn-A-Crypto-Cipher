{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from solution import get_cipher_key, get_random_pairs, encode_output, get_model, get_alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Can we learn an AES cipher?***\n",
    "\n",
    "I became curious one day if you could learn a cipher if you had pairs of inputs and outputs. I thought to myself 'AES is a thing, right?' and decided to see if I could train a neural net to learn an AES cipher if we knew ahead of time all the origin and encrypted text.\n",
    "\n",
    "If you're interested, there is some additional code behind the notebook available in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start off by generating some training data. Here we'll generate 50000 examples of encrypted text and print a few examples. The generated text is just random characters. In reality, you don't stand a chance of learning an AES cipher with this little data, but here we go anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key:  b'n\\xea\\xb6\\xcb\\x13\\x1c\\x17,n\\x15\\xa2g\\xe4\\x9f\\xeb}'\n",
      "Generated 250000 encrpyted sequences.\n",
      "Example of origin and encrypted text:\n",
      "\t^Wn#SrSLs\n",
      "\t\\xb5\\xc1A\\x8c\\xa1{ B\\xd\n",
      "Example of origin and encrypted text:\n",
      "\tM hwgeZ@e\n",
      "\t\\x8cV>sjE)\\x1e\n",
      "Example of origin and encrypted text:\n",
      "\tJSgW@Av\n",
      "\tM\\xdbW\\xbf\\x0c\\xab\\x9\n"
     ]
    }
   ],
   "source": [
    "# generate a key we can use to create a cipher and encode the text, and then print the key\n",
    "key = get_cipher_key()\n",
    "print('The key: ', key)\n",
    "\n",
    "# get our training data\n",
    "text, ciphertext = get_random_pairs(250000, 5, 10, key)\n",
    "\n",
    "# this should just be the length we asked for\n",
    "print('Generated {0} encrpyted sequences.'.format(len(ciphertext)))\n",
    "\n",
    "# print 3 examples\n",
    "for i in range(1000,1003):\n",
    "    print('Example of origin and encrypted text:')\n",
    "    print('\\t{0}'.format(text[i]))\n",
    "    print('\\t{0}'.format(ciphertext[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create the training data***\n",
    "\n",
    "Now we need to calculate some required data so we can encode the text for the model, and then actually encode the text. The input will be int encoded because I'm going to try an embedding layer, and the output will be one-hot encoded to predict the characters in the output sequence. The model will be a basic seq2seq LSTM modeled in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# turn the lists into numpy arrays\n",
    "df_text = pd.DataFrame(text, columns=['text'])\n",
    "np_text = df_text.to_numpy()\n",
    "df_ciphertext = pd.DataFrame(ciphertext, columns=['ciphertext'])\n",
    "np_ciphertext = df_ciphertext.to_numpy()\n",
    "\n",
    "# determine what unique characters are present in the ciphertext\n",
    "ct_alphabet = set()\n",
    "for line in ciphertext:\n",
    "    [ct_alphabet.add(c) for c in line]\n",
    "\n",
    "# determine the maximum length of any sequence in the text and ciphertext\n",
    "max_ct_len = max([len(line) for line in ciphertext])\n",
    "max_input_len = max([len(line) for line in text])\n",
    "\n",
    "# create char<->index and index<->char dictionaries\n",
    "alphabet = get_alphabet()\n",
    "ctalph_to_idx = { char: i+1 for i, char in enumerate(ct_alphabet) }\n",
    "idx_to_ctalph = { ctalph_to_idx[key]: key for key in ctalph_to_idx.keys() }\n",
    "alph_to_idx = { char: i+1 for i, char in enumerate(alphabet) }\n",
    "idx_to_alph = { alph_to_idx[key]: key for key in alph_to_idx.keys() }\n",
    "\n",
    "# int encode all the text and ciphertext\n",
    "encoded_text_lines = []\n",
    "for i, line in enumerate(np_text):\n",
    "    line = line[0]\n",
    "    new_line = np.zeros((len(line), ))\n",
    "    for j, char in enumerate(line):\n",
    "        new_line[j] = alph_to_idx[char]\n",
    "    encoded_text_lines.append(new_line)\n",
    "np_text = np.asarray(encoded_text_lines)\n",
    "\n",
    "encoded_text_lines = []\n",
    "for i, line in enumerate(np_ciphertext):\n",
    "    line = line[0]\n",
    "    new_line = np.zeros((len(line), ))    \n",
    "    for j, char in enumerate(line):\n",
    "        new_line[j] = ctalph_to_idx[char]       \n",
    "    encoded_text_lines.append(new_line)\n",
    "np_ciphertext = np.asarray(encoded_text_lines)\n",
    "\n",
    "# ensure that all sequences are the same length by applying zero padding to increase the length of shorter sequences\n",
    "np_text = pad_sequences(np_text, maxlen=max_input_len, padding='pre')\n",
    "np_ciphertext = pad_sequences(np_ciphertext, maxlen=max_ct_len, padding='pre')\n",
    "\n",
    "# determine how many unique characters exist in the input and output sequences\n",
    "alphabet_len = len(alphabet) +1 # +1 to accommodate the padding char which was not in the original alphabet\n",
    "ct_alphabet_len = len(ct_alphabet) +1 # +1 here too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prepare to train the model***\n",
    "\n",
    "Ok now the fun part (jk I love it all) where we try to train the model. Here we'll finalize the training data and create the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35, 3)             288       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               532480    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 9, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 9, 256)            787456    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 9, 78)             20046     \n",
      "=================================================================\n",
      "Total params: 1,340,270\n",
      "Trainable params: 1,340,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# use the traditional X, y variable names\n",
    "y = encode_output(np_text, alphabet_len, alph_to_idx)\n",
    "X = np_ciphertext\n",
    "\n",
    "# get the lengths of the input and output sequences\n",
    "input_seq_len = X.shape[1]\n",
    "output_seq_len = y.shape[1]\n",
    "\n",
    "# get an instance of the model and use a small embedding dim\n",
    "embedding_dim = 3\n",
    "model = get_model(alphabet_len, ct_alphabet_len, input_seq_len, output_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then see how it goes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jarre\\Anaconda3\\envs\\ML 2.0\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 50000 samples\n",
      "Epoch 1/50\n",
      "200000/200000 [==============================] - 44s 219us/step - loss: 3.5617 - accuracy: 0.2290 - val_loss: 3.4459 - val_accuracy: 0.2311\n",
      "Epoch 2/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.4299 - accuracy: 0.2318 - val_loss: 3.4146 - val_accuracy: 0.2315\n",
      "Epoch 3/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.4089 - accuracy: 0.2321 - val_loss: 3.4074 - val_accuracy: 0.2318\n",
      "Epoch 4/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.4244 - accuracy: 0.2316 - val_loss: 3.4094 - val_accuracy: 0.2315\n",
      "Epoch 5/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.4027 - accuracy: 0.2323 - val_loss: 3.4043 - val_accuracy: 0.2317\n",
      "Epoch 6/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.4075 - accuracy: 0.2320 - val_loss: 3.4007 - val_accuracy: 0.2316\n",
      "Epoch 7/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3934 - accuracy: 0.2323 - val_loss: 3.3944 - val_accuracy: 0.2317\n",
      "Epoch 8/50\n",
      "200000/200000 [==============================] - 42s 210us/step - loss: 3.3904 - accuracy: 0.2324 - val_loss: 3.3923 - val_accuracy: 0.2316\n",
      "Epoch 9/50\n",
      "200000/200000 [==============================] - 42s 210us/step - loss: 3.3885 - accuracy: 0.2325 - val_loss: 3.3915 - val_accuracy: 0.2320\n",
      "Epoch 10/50\n",
      "200000/200000 [==============================] - 42s 209us/step - loss: 3.3981 - accuracy: 0.2320 - val_loss: 3.3924 - val_accuracy: 0.2315\n",
      "Epoch 11/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3931 - accuracy: 0.2323 - val_loss: 3.3892 - val_accuracy: 0.2319\n",
      "Epoch 12/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.3856 - accuracy: 0.2326 - val_loss: 3.3881 - val_accuracy: 0.2317\n",
      "Epoch 13/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3846 - accuracy: 0.2326 - val_loss: 3.3874 - val_accuracy: 0.2315\n",
      "Epoch 14/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3834 - accuracy: 0.2326 - val_loss: 3.3869 - val_accuracy: 0.2317\n",
      "Epoch 15/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3828 - accuracy: 0.2324 - val_loss: 3.3858 - val_accuracy: 0.2317\n",
      "Epoch 16/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.4337 - accuracy: 0.2298 - val_loss: 3.3943 - val_accuracy: 0.2310\n",
      "Epoch 17/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3946 - accuracy: 0.2321 - val_loss: 3.3879 - val_accuracy: 0.2319\n",
      "Epoch 18/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.3827 - accuracy: 0.2325 - val_loss: 3.3855 - val_accuracy: 0.2315\n",
      "Epoch 19/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3992 - accuracy: 0.2317 - val_loss: 3.3872 - val_accuracy: 0.2315\n",
      "Epoch 20/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3823 - accuracy: 0.2325 - val_loss: 3.3847 - val_accuracy: 0.2320\n",
      "Epoch 21/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3810 - accuracy: 0.2326 - val_loss: 3.3844 - val_accuracy: 0.2316\n",
      "Epoch 22/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3804 - accuracy: 0.2328 - val_loss: 3.3848 - val_accuracy: 0.2317\n",
      "Epoch 23/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.3800 - accuracy: 0.2327 - val_loss: 3.3836 - val_accuracy: 0.2316\n",
      "Epoch 24/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3796 - accuracy: 0.2329 - val_loss: 3.3856 - val_accuracy: 0.2318\n",
      "Epoch 25/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3877 - accuracy: 0.2324 - val_loss: 3.3858 - val_accuracy: 0.2316\n",
      "Epoch 26/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3802 - accuracy: 0.2329 - val_loss: 3.3836 - val_accuracy: 0.2316\n",
      "Epoch 27/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3791 - accuracy: 0.2329 - val_loss: 3.3832 - val_accuracy: 0.2317\n",
      "Epoch 28/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3788 - accuracy: 0.2330 - val_loss: 3.3830 - val_accuracy: 0.2317\n",
      "Epoch 29/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3787 - accuracy: 0.2329 - val_loss: 3.3832 - val_accuracy: 0.2316\n",
      "Epoch 30/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3785 - accuracy: 0.2331 - val_loss: 3.3838 - val_accuracy: 0.2317\n",
      "Epoch 31/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3784 - accuracy: 0.2331 - val_loss: 3.3830 - val_accuracy: 0.2316\n",
      "Epoch 32/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3782 - accuracy: 0.2331 - val_loss: 3.3834 - val_accuracy: 0.2318\n",
      "Epoch 33/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.3780 - accuracy: 0.2332 - val_loss: 3.3834 - val_accuracy: 0.2320\n",
      "Epoch 34/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.3779 - accuracy: 0.2333 - val_loss: 3.3836 - val_accuracy: 0.2320\n",
      "Epoch 35/50\n",
      "200000/200000 [==============================] - 42s 212us/step - loss: 3.3825 - accuracy: 0.2331 - val_loss: 3.3832 - val_accuracy: 0.2319\n",
      "Epoch 36/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3773 - accuracy: 0.2333 - val_loss: 3.3832 - val_accuracy: 0.2317\n",
      "Epoch 37/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3770 - accuracy: 0.2336 - val_loss: 3.3835 - val_accuracy: 0.2318\n",
      "Epoch 38/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3768 - accuracy: 0.2335 - val_loss: 3.3839 - val_accuracy: 0.2319\n",
      "Epoch 39/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3766 - accuracy: 0.2338 - val_loss: 3.3841 - val_accuracy: 0.2321\n",
      "Epoch 40/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3762 - accuracy: 0.2337 - val_loss: 3.3840 - val_accuracy: 0.2318\n",
      "Epoch 41/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3760 - accuracy: 0.2338 - val_loss: 3.3842 - val_accuracy: 0.2317\n",
      "Epoch 42/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3756 - accuracy: 0.2339 - val_loss: 3.3850 - val_accuracy: 0.2318\n",
      "Epoch 43/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3856 - accuracy: 0.2331 - val_loss: 3.3843 - val_accuracy: 0.2316\n",
      "Epoch 44/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3751 - accuracy: 0.2341 - val_loss: 3.3853 - val_accuracy: 0.2318\n",
      "Epoch 45/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3745 - accuracy: 0.2341 - val_loss: 3.3855 - val_accuracy: 0.2320\n",
      "Epoch 46/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3740 - accuracy: 0.2343 - val_loss: 3.3858 - val_accuracy: 0.2316\n",
      "Epoch 47/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3735 - accuracy: 0.2345 - val_loss: 3.3860 - val_accuracy: 0.2317\n",
      "Epoch 48/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3730 - accuracy: 0.2347 - val_loss: 3.3867 - val_accuracy: 0.2319\n",
      "Epoch 49/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3725 - accuracy: 0.2349 - val_loss: 3.3872 - val_accuracy: 0.2319\n",
      "Epoch 50/50\n",
      "200000/200000 [==============================] - 42s 211us/step - loss: 3.3720 - accuracy: 0.2349 - val_loss: 3.3879 - val_accuracy: 0.2316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cbb667cf60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=50\n",
    "batch_size = 512\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hmm.. so that didn't work**\n",
    "\n",
    "Ok so that did not work (actually as expected). AES does (more-or-less) secure the whole internet so thankfully we cannot use simple seq2seq model to learn the AES cipher when we also don't know the key. There is a little bit of accuracy here (22-23%) but that is only due to correctly guessing the padding char, not actually decoding the sequence.\n",
    "\n",
    "\n",
    "**Round 2**\n",
    "\n",
    "Now I'll try including the key with the encoded text, as input to the model, and see if it can learn an AES cipher when the decryption key is known ahead of time. I'll use the same seq2seq LSTM model as before. We just need to re-create the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 83)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 83, 3)             288       \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               532480    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 9, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 9, 256)            787456    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 9, 78)             20046     \n",
      "=================================================================\n",
      "Total params: 1,340,270\n",
      "Trainable params: 1,340,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the only difference is we're setting use_key=True when creating the training data\n",
    "text, ciphertext = get_random_pairs(250000, 5, 10, key, use_key=True)\n",
    "\n",
    "# turn the lists into numpy arrays\n",
    "df_text = pd.DataFrame(text, columns=['text'])\n",
    "np_text = df_text.to_numpy()\n",
    "df_ciphertext = pd.DataFrame(ciphertext, columns=['ciphertext'])\n",
    "np_ciphertext = df_ciphertext.to_numpy()\n",
    "\n",
    "# determine what unique characters are present in the ciphertext\n",
    "ct_alphabet = set()\n",
    "for line in ciphertext:\n",
    "    [ct_alphabet.add(c) for c in line]\n",
    "\n",
    "# determine the maximum length of any sequence in the text and ciphertext\n",
    "max_ct_len = max([len(line) for line in ciphertext])\n",
    "max_input_len = max([len(line) for line in text])\n",
    "\n",
    "# create char<->index and index<->char dictionaries\n",
    "alphabet = get_alphabet()\n",
    "ctalph_to_idx = { char: i+1 for i, char in enumerate(ct_alphabet) }\n",
    "idx_to_ctalph = { ctalph_to_idx[key]: key for key in ctalph_to_idx.keys() }\n",
    "alph_to_idx = { char: i+1 for i, char in enumerate(alphabet) }\n",
    "idx_to_alph = { alph_to_idx[key]: key for key in alph_to_idx.keys() }\n",
    "\n",
    "# int encode all the text and ciphertext\n",
    "encoded_text_lines = []\n",
    "for i, line in enumerate(np_text):\n",
    "    line = line[0]\n",
    "    new_line = np.zeros((len(line), ))\n",
    "    for j, char in enumerate(line):\n",
    "        new_line[j] = alph_to_idx[char]\n",
    "    encoded_text_lines.append(new_line)\n",
    "np_text = np.asarray(encoded_text_lines)\n",
    "\n",
    "encoded_text_lines = []\n",
    "for i, line in enumerate(np_ciphertext):\n",
    "    line = line[0]\n",
    "    new_line = np.zeros((len(line), ))    \n",
    "    for j, char in enumerate(line):\n",
    "        new_line[j] = ctalph_to_idx[char]       \n",
    "    encoded_text_lines.append(new_line)\n",
    "np_ciphertext = np.asarray(encoded_text_lines)\n",
    "\n",
    "# ensure that all sequences are the same length by applying zero padding to increase the length of shorter sequences\n",
    "np_text = pad_sequences(np_text, maxlen=max_input_len, padding='pre')\n",
    "np_ciphertext = pad_sequences(np_ciphertext, maxlen=max_ct_len, padding='pre')\n",
    "\n",
    "# determine how many unique characters exist in the input and output sequences\n",
    "alphabet_len = len(alphabet) +1 # +1 to accommodate the padding char which was not in the original alphabet\n",
    "ct_alphabet_len = len(ct_alphabet) +1 # +1 here too\n",
    "\n",
    "# use the traditional X, y variable names\n",
    "y = encode_output(np_text, alphabet_len, alph_to_idx)\n",
    "X = np_ciphertext\n",
    "\n",
    "# get the lengths of the input and output sequences\n",
    "input_seq_len = X.shape[1]\n",
    "output_seq_len = y.shape[1]\n",
    "\n",
    "# get an instance of the model and use a small embedding dim\n",
    "embedding_dim = 3\n",
    "model = get_model(alphabet_len, ct_alphabet_len, input_seq_len, output_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Peek at the training data**\n",
    "\n",
    "If we remove the zero padding from each string and print it, we can see that each training example starts with the same sequence, which is the key to the cipher.\n",
    "\n",
    "[94, 51, 7, 33, 27, 51, 7, 16, 38, 51...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row 0:  [94, 51, 7, 33, 27, 51, 7, 16, 38, 51, 7, 50, 16, 51, 7, 60, 47, 51, 7, 60, 50, 51, 7, 60, 85, 78, 94, 51, 7, 60, 88, 51, 7, 27, 67, 9, 51, 7, 33, 17, 51, 7, 77, 26, 51, 7, 33, 16, 28, 19, 51, 7, 60, 15, 40, 51, 7, 27]\n",
      "\n",
      "Row 1:  [94, 51, 7, 33, 27, 51, 7, 16, 38, 51, 7, 50, 16, 51, 7, 60, 47, 51, 7, 60, 50, 51, 7, 60, 85, 78, 94, 51, 7, 60, 88, 51, 7, 27, 67, 9, 51, 7, 33, 17, 51, 7, 77, 26, 51, 7, 33, 16, 51, 7, 60, 60, 9, 14, 51, 7, 93, 38, 63, 51, 7, 3]\n",
      "\n",
      "Row 2:  [94, 51, 7, 33, 27, 51, 7, 16, 38, 51, 7, 50, 16, 51, 7, 60, 47, 51, 7, 60, 50, 51, 7, 60, 85, 78, 94, 51, 7, 60, 88, 51, 7, 27, 67, 9, 51, 7, 33, 17, 51, 7, 77, 26, 51, 7, 33, 16, 51, 7, 85, 26, 51, 7, 27, 16, 51, 7, 77, 85, 87, 51, 7, 16, 85, 51, 7, 33, 17, 51, 7, 33, 27, 51, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(X[:3, :]): print(f'\\nRow {i}: ', list(filter(lambda char: char != 0, row)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now train!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jarre\\Anaconda3\\envs\\ML 2.0\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 50000 samples\n",
      "Epoch 1/50\n",
      "200000/200000 [==============================] - 89s 443us/step - loss: 3.7036 - accuracy: 0.2267 - val_loss: 3.6437 - val_accuracy: 0.2297\n",
      "Epoch 2/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.6453 - accuracy: 0.2290 - val_loss: 3.6404 - val_accuracy: 0.2298\n",
      "Epoch 3/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.6442 - accuracy: 0.2291 - val_loss: 3.6404 - val_accuracy: 0.2298\n",
      "Epoch 4/50\n",
      "200000/200000 [==============================] - 87s 434us/step - loss: 3.6437 - accuracy: 0.2290 - val_loss: 3.6435 - val_accuracy: 0.2297\n",
      "Epoch 5/50\n",
      "200000/200000 [==============================] - 87s 436us/step - loss: 3.6433 - accuracy: 0.2290 - val_loss: 3.6410 - val_accuracy: 0.2297\n",
      "Epoch 6/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.6427 - accuracy: 0.2291 - val_loss: 3.6376 - val_accuracy: 0.2295\n",
      "Epoch 7/50\n",
      "200000/200000 [==============================] - 88s 438us/step - loss: 3.6436 - accuracy: 0.2289 - val_loss: 3.6475 - val_accuracy: 0.2300\n",
      "Epoch 8/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.6017 - accuracy: 0.2290 - val_loss: 3.5026 - val_accuracy: 0.2307\n",
      "Epoch 9/50\n",
      "200000/200000 [==============================] - 87s 436us/step - loss: 3.4816 - accuracy: 0.2306 - val_loss: 3.4705 - val_accuracy: 0.2316\n",
      "Epoch 10/50\n",
      "200000/200000 [==============================] - 88s 438us/step - loss: 3.4650 - accuracy: 0.2309 - val_loss: 3.4484 - val_accuracy: 0.2319\n",
      "Epoch 11/50\n",
      "200000/200000 [==============================] - 88s 438us/step - loss: 3.4489 - accuracy: 0.2311 - val_loss: 3.4284 - val_accuracy: 0.2319\n",
      "Epoch 12/50\n",
      "200000/200000 [==============================] - 88s 438us/step - loss: 3.4436 - accuracy: 0.2310 - val_loss: 3.4245 - val_accuracy: 0.2320\n",
      "Epoch 13/50\n",
      "200000/200000 [==============================] - 88s 438us/step - loss: 3.4157 - accuracy: 0.2314 - val_loss: 3.3937 - val_accuracy: 0.2324\n",
      "Epoch 14/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.3942 - accuracy: 0.2318 - val_loss: 3.3865 - val_accuracy: 0.2326\n",
      "Epoch 15/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.4022 - accuracy: 0.2314 - val_loss: 3.3855 - val_accuracy: 0.2326\n",
      "Epoch 16/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.4190 - accuracy: 0.2306 - val_loss: 3.3873 - val_accuracy: 0.2325\n",
      "Epoch 17/50\n",
      "200000/200000 [==============================] - 87s 436us/step - loss: 3.3880 - accuracy: 0.2319 - val_loss: 3.3837 - val_accuracy: 0.2324\n",
      "Epoch 18/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.4156 - accuracy: 0.2307 - val_loss: 3.4097 - val_accuracy: 0.2327\n",
      "Epoch 19/50\n",
      "200000/200000 [==============================] - 88s 440us/step - loss: 3.4032 - accuracy: 0.2312 - val_loss: 3.3837 - val_accuracy: 0.2327\n",
      "Epoch 20/50\n",
      "200000/200000 [==============================] - 88s 438us/step - loss: 3.4211 - accuracy: 0.2303 - val_loss: 3.3911 - val_accuracy: 0.2325\n",
      "Epoch 21/50\n",
      "200000/200000 [==============================] - 88s 438us/step - loss: 3.3877 - accuracy: 0.2318 - val_loss: 3.3833 - val_accuracy: 0.2327\n",
      "Epoch 22/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.3848 - accuracy: 0.2321 - val_loss: 3.3815 - val_accuracy: 0.2324\n",
      "Epoch 23/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.4354 - accuracy: 0.2294 - val_loss: 3.3829 - val_accuracy: 0.2322\n",
      "Epoch 24/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.3846 - accuracy: 0.2318 - val_loss: 3.3805 - val_accuracy: 0.2326\n",
      "Epoch 25/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.3832 - accuracy: 0.2321 - val_loss: 3.3800 - val_accuracy: 0.2326\n",
      "Epoch 26/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.4069 - accuracy: 0.2309 - val_loss: 3.3816 - val_accuracy: 0.2323\n",
      "Epoch 27/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.3830 - accuracy: 0.2320 - val_loss: 3.3797 - val_accuracy: 0.2327\n",
      "Epoch 28/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.3820 - accuracy: 0.2320 - val_loss: 3.3793 - val_accuracy: 0.2326\n",
      "Epoch 29/50\n",
      "200000/200000 [==============================] - 87s 434us/step - loss: 3.3817 - accuracy: 0.2321 - val_loss: 3.3789 - val_accuracy: 0.2327\n",
      "Epoch 30/50\n",
      "200000/200000 [==============================] - 87s 434us/step - loss: 3.4318 - accuracy: 0.2282 - val_loss: 3.3808 - val_accuracy: 0.2326\n",
      "Epoch 31/50\n",
      "200000/200000 [==============================] - 87s 436us/step - loss: 3.3826 - accuracy: 0.2319 - val_loss: 3.3793 - val_accuracy: 0.2326\n",
      "Epoch 32/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.4498 - accuracy: 0.2268 - val_loss: 3.3881 - val_accuracy: 0.2323\n",
      "Epoch 33/50\n",
      "200000/200000 [==============================] - 87s 435us/step - loss: 3.3847 - accuracy: 0.2320 - val_loss: 3.3798 - val_accuracy: 0.2325\n",
      "Epoch 34/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.3821 - accuracy: 0.2323 - val_loss: 3.3790 - val_accuracy: 0.2327\n",
      "Epoch 35/50\n",
      "200000/200000 [==============================] - 87s 436us/step - loss: 3.3814 - accuracy: 0.2321 - val_loss: 3.3790 - val_accuracy: 0.2325\n",
      "Epoch 36/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.3812 - accuracy: 0.2322 - val_loss: 3.3786 - val_accuracy: 0.2324\n",
      "Epoch 37/50\n",
      "200000/200000 [==============================] - 88s 442us/step - loss: 3.3809 - accuracy: 0.2324 - val_loss: 3.3787 - val_accuracy: 0.2328\n",
      "Epoch 38/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.3809 - accuracy: 0.2322 - val_loss: 3.3788 - val_accuracy: 0.2326\n",
      "Epoch 39/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.3808 - accuracy: 0.2323 - val_loss: 3.3787 - val_accuracy: 0.2329\n",
      "Epoch 40/50\n",
      "200000/200000 [==============================] - 88s 441us/step - loss: 3.3807 - accuracy: 0.2323 - val_loss: 3.3785 - val_accuracy: 0.2328\n",
      "Epoch 41/50\n",
      "200000/200000 [==============================] - 87s 437us/step - loss: 3.3805 - accuracy: 0.2324 - val_loss: 3.3787 - val_accuracy: 0.2324\n",
      "Epoch 42/50\n",
      "200000/200000 [==============================] - 88s 440us/step - loss: 3.3805 - accuracy: 0.2325 - val_loss: 3.3788 - val_accuracy: 0.2323\n",
      "Epoch 43/50\n",
      "200000/200000 [==============================] - 88s 441us/step - loss: 3.4934 - accuracy: 0.2289 - val_loss: 3.4218 - val_accuracy: 0.2322\n",
      "Epoch 44/50\n",
      "200000/200000 [==============================] - 88s 440us/step - loss: 3.4332 - accuracy: 0.2310 - val_loss: 3.4625 - val_accuracy: 0.2320\n",
      "Epoch 45/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.4022 - accuracy: 0.2316 - val_loss: 3.3836 - val_accuracy: 0.2324\n",
      "Epoch 46/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.3957 - accuracy: 0.2316 - val_loss: 3.3846 - val_accuracy: 0.2325\n",
      "Epoch 47/50\n",
      "200000/200000 [==============================] - 88s 441us/step - loss: 3.3890 - accuracy: 0.2320 - val_loss: 3.3804 - val_accuracy: 0.2327\n",
      "Epoch 48/50\n",
      "200000/200000 [==============================] - 88s 440us/step - loss: 3.3824 - accuracy: 0.2323 - val_loss: 3.3796 - val_accuracy: 0.2329\n",
      "Epoch 49/50\n",
      "200000/200000 [==============================] - 88s 439us/step - loss: 3.4352 - accuracy: 0.2305 - val_loss: 3.4140 - val_accuracy: 0.2319\n",
      "Epoch 50/50\n",
      "200000/200000 [==============================] - 88s 440us/step - loss: 3.3951 - accuracy: 0.2321 - val_loss: 3.3821 - val_accuracy: 0.2326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cc33bd3f28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=50\n",
    "batch_size = 512\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This concludes the test**\n",
    "\n",
    "Well, if you needed any convincing that you cannot learn to decipher AES with a small LSTM and few training examples, there you have it.  :)\n",
    "\n",
    "Once again we're seeing just 22-23% accuracy, and even that is only due to correctly guessing the padding character. No meaningful predictions were possible. \n",
    "\n",
    "Check the other notebook in this repo to see an simple cipher that can be learned with an LSTM!\n",
    "\n",
    "**Thanks for following along!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
